import torch
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import librosa
import matplotlib.pyplot as plt

from models.HidingUNet import UnetGenerator  # Import the Hiding model

# Load the trained hiding model
def load_hiding_model():
    model = UnetGenerator(input_nc=6, output_nc=3, num_downs=7, output_function=torch.nn.Sigmoid)
    model.load_state_dict(torch.load("./checkPoint/netH_epoch_73,sumloss=0.000447,Hloss=0.000258.pth", map_location="cpu"))
    model.eval()
    return model

hiding_model = load_hiding_model()

# Image transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

def audio_to_image(audio_path, audio_image_path):
    y, sr = librosa.load(audio_path, sr=None)

    # Normalize audio samples from [-1, 1] to [0, 1]
    y_norm = (y + 1.0) / 2.0

    # Total number of pixel values required for a 256x256 RGB image
    target_size = 256 * 256 * 3  # 196608 values

    # If the audio is shorter than required, pad with zeros; if longer, truncate it
    if len(y_norm) < target_size:
        y_norm = np.pad(y_norm, (0, target_size - len(y_norm)), mode='constant')
    else:
        y_norm = y_norm[:target_size]

    # Reshape the 1D audio array into a (256, 256, 3) array
    img_array = np.reshape(y_norm, (256, 256, 3))

    # Convert normalized values to 8-bit pixel intensities [0, 255]
    img_array = (img_array * 255).astype(np.uint8)

    # Create and save the image
    img = Image.fromarray(img_array)
    img.save(audio_image_path)

    return audio_image_path

def embed_image(cover_img_path, audio_img_path, stego_img_path):
    """
    Takes a cover image and an audio image (generated by the new audio_to_image function),
    and embeds the audio image inside the cover image.
    """
    cover_img = Image.open(cover_img_path).convert("RGB")
    secret_img = Image.open(audio_img_path).convert("RGB")

    # Apply transformations
    cover_tensor = transform(cover_img).unsqueeze(0)
    secret_tensor = transform(secret_img).unsqueeze(0)

    # Concatenate the images along the channel dimension
    concat_tensor = torch.cat([cover_tensor, secret_tensor], dim=1)

    with torch.no_grad():
        container_tensor = hiding_model(concat_tensor)

    # Convert the output tensor back to an image and save it
    container_img = transforms.ToPILImage()(container_tensor.squeeze(0))
    container_img.save(stego_img_path)

    return stego_img_path
